---
title: "Chapter 15: Other generalized linear models"
author: "Wang minjie"
date: "`r format(Sys.Date())`"
output:
  github_document
---


```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidybayes)
library(bayesplot)
library(rstan)
library(patchwork)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(bayesplot::theme_default())
```


### 15.2.1 Poisson model.

```{r, warning = F, message = F}
n <- 50

a <- 1
b <- 2

fake <- 
  tibble(x = runif(n, -2, 2)) %>% 
  mutate(y = rpois(n, lambda = exp(a + b * x)))

head(fake)
```






```{r, warning=FALSE, message=FALSE}
stan_program <- "
data {
  int<lower=0> N;
  int<lower=0> K;
  matrix[N, K] X;
  int<lower=0> y[N];
}

parameters {
  vector[K] beta;
}

model {
  // y ~ poisson_log( X * beta);
  target += poisson_log_lpmf(y | X * beta);
}
"


stan_data <- fake %>%
  tidybayes::compose_data(
    N = n,
    K = 2,
    y = fake,
    X = model.matrix(~ 1 + x)
  )

m15.1 <- stan(model_code = stan_program, data = stan_data)
```

```{r}
m15.1
```


### 15.2.8 Example: zeroes in count data.

```{r, warning = F}
roaches <- 
  read_csv("ROS-Examples/Roaches/data/roaches.csv", 
           col_types = cols(X1 = col_skip())) %>% 
  mutate(roach100 = roach1 / 100)

glimpse(roaches)
```


```{r, warning=FALSE, message=FALSE}
stan_program <- "
data {
  int<lower=0> N;
  int<lower=0> K;
  matrix[N, K] X;
  int<lower=0> y[N];
  vector[N] offset;
}

parameters {
  vector[K] beta;
  real phi;
}

model {
  //target += neg_binomial_2_log_lpmf(y| X * beta, phi);
  
  for(i in 1:N) {
    y[i] ~ neg_binomial_2(offset[i] * exp(X[i] * beta), phi);
  }
}

generated quantities {
  int y_rep[N];
  for (n in 1:N) {
    y_rep[n] = neg_binomial_2_rng(offset[n] * exp(X[n] * beta), phi);
  }
}

"


stan_data <- roaches %>%
  tidybayes::compose_data(
    N      = n,
    K      = 4,
    y      = y,
    X      = model.matrix(~ 1 + roach100 + treatment + senior),
    offset = exposure2
  )

m15.2 <- stan(model_code = stan_program, data = stan_data)
```



```{r}
m15.2
```


```{r}
y_rep <- as.matrix(m15.2, pars = "y_rep")
bayesplot::ppc_dens_overlay(y = log10(stan_data$y + 1), yrep = log10(y_rep[1:200,] + 1))
```


```{r}
m15.2 %>% 
  tidybayes::gather_draws(y_rep[i], ndraws = 200) %>% 
  ungroup() %>% 
  mutate(y = log10(.value + 1) ) %>% 

  ggplot(aes(x = y)) +
  geom_density(aes(group = .draw), color = "gray50") +
  geom_density(
    data = roaches %>% mutate(y = log10(y + 1)),
    color = "red"
  ) 
```

### What if we had used Poisson regression?

```{r, warning=FALSE, message=FALSE}
stan_program <- "
data {
  int<lower=0> N;
  int<lower=0> K;
  matrix[N, K] X;
  int<lower=0> y[N];
  vector[N] offset;
}

parameters {
  vector[K] beta;
}

model {

  for(i in 1:N) {
    y[i] ~ poisson(offset[i] * exp(X[i] * beta));
  }
  
}

generated quantities {
  int y_rep[N];
  for (n in 1:N) {
    y_rep[n] = poisson_rng(offset[n] * exp(X[n] * beta));
  }
}

"


stan_data <- roaches %>%
  tidybayes::compose_data(
    N      = n,
    K      = 4,
    y      = y,
    X      = model.matrix(~ 1 + roach100 + treatment + senior),
    offset = exposure2
  )

m15.3 <- stan(model_code = stan_program, data = stan_data)
```

```{r}
m15.3
```


```{r}
y_rep <- as.matrix(m15.3, pars = "y_rep")
bayesplot::ppc_dens_overlay(y = log10(stan_data$y + 1), yrep = log10(y_rep[1:200,] + 1))
```


## 15.3 Logistic-binomial model

100个选手每人投篮20次，假定命中概率是身高的线性函数

```{r}
n <- 100

data <-
  tibble(size   = 20,
         height = rnorm(n, mean = 72, sd = 3)) %>% 
  mutate(y = rbinom(n, size = size, p = 0.4 + 0.1 * (height - 72) / 3))

head(data)
```

$$
\begin{align*}
y_i & = \text{Binomial}(n_i, p_i) \\
p_i & =\text{logit}^{-1}(X_i \beta) 
\end{align*}
$$

```{r, warning=FALSE, message=FALSE}
stan_program <- "
data {
  int<lower=0> N;
  int<lower=0> K;
  matrix[N, K] X;
  int<lower=0> y[N];
  int trials[N];
}

parameters {
  vector[K] beta;
}

model {
  
  for(i in 1:N) {
    target += binomial_logit_lpmf(y[i] | trials[i], X[i] * beta);
  }
  
}


"


stan_data <- data %>%
  tidybayes::compose_data(
    N      = n,
    K      = 2,
    y      = y,
    trials = size,
    X      = model.matrix(~ 1 + height)
  )

m15.5 <- stan(model_code = stan_program, data = stan_data)
```


```{r}
m15.5
```


### 15.3.1 The binomial model for count data, applied to death sentences.

```{r}
death <- haven::read_sas("ROS-Examples/death_penalty_data/a14.sas7bdat", NULL)

dim(death)
```


```{r}
death %>% 
  filter(TOTLDF >= 1) %>% 
  dplyr::select(CNTRELF, TOTLDF, YEAR, STATE)
```


```{r}
death <- death %>% 
  dplyr::filter(TOTLDF >= 1) %>% 
  dplyr::select(CNTRELF, TOTLDF, YEAR, STATE) %>% 
  dplyr::rename_with(tolower) %>% 
  dplyr::mutate(
    year1 = year - 1973 + 1,
    year_c = year - 1984
  ) %>% 
  dplyr::mutate(state_num = as.numeric(factor(state)))

head(death)
```




```{r}
# `state_al` as baseline
model.matrix(~ 1 + state + year_c, data = death) %>% 
  colnames()
```



```{r, warning=FALSE, message=FALSE}
stan_program <- "
data {
  int<lower=0> N;
  int<lower=0> K;
  matrix[N, K] X;
  int<lower=0> y[N];
  int trials[N];
}

parameters {
  vector[K] beta;
}

model {
  
  for(i in 1:N) {
    target += binomial_logit_lpmf(y[i] | trials[i], X[i] * beta);
  }
  
}


"


stan_data <- list(
    N      = nrow(death),
    y      = death$cntrelf,
    trials = death$totldf,
    X      = model.matrix(~ 1 + state + year_c, data = death),
    K      = length(unique(death$state)) - 1 + 2
  )

m15.6 <- stan(model_code = stan_program, data = stan_data)
```


```{r}
m15.6
```


```{r m15.6}
library(brms)
b15.6 <- 
  brm(data = death, 
      family = binomial,
      cntrelf | trials(totldf) ~ state + year_c,
      seed = 15)
```


```{r}
b15.6
```


作者Gelmen说，最好用多层模型(p. 271)，这里截距和时间斜率加入变化效应


```{r, warning=FALSE, message=FALSE}
stan_program <- "
data {
  int<lower=0> N;
  int<lower=0> K;
  matrix[N, K] X;
  int<lower=0> y[N];
  int trials[N];
  int J;                      // number of grouping
  int<lower=1, upper=J> g[N]; // index for grouping
}

parameters {
  array[J] vector[K] beta;
  vector[K] MU;
  vector<lower=0>[K] tau;
  corr_matrix[K] Rho;
}

model {
  vector[N] mu;
  
  for(i in 1:N) {
    mu[i] = X[i] * beta[g[i]];
  }
  
  for(i in 1:N) {
    target += binomial_logit_lpmf(y[i] | trials[i], mu[i]);
  }
  
  beta ~ multi_normal(MU, quad_form_diag(Rho, tau));
  tau ~ exponential(1);
  Rho ~ lkj_corr(2);
}


"


stan_data <- list(
    N      = nrow(death),
    K      = 2,
    y      = death$cntrelf,
    trials = death$totldf,
    X      = model.matrix(~ 1 + year_c, data = death),
    J      = length(unique(death$state)),
    g      = death$state_num
  )

m15.7 <- stan(model_code = stan_program, data = stan_data)
```




### 15.5.3 Example of ordered categorical regression.

Load the `2playergames.csv`, `3playergames.csv`, and `6playergames.csv` data files.

```{r, message = F}
# load the individual data files
data_2player <- read_csv("ROS-Examples/Storable/data/2playergames.csv")
data_3player <- read_csv("ROS-Examples/Storable/data/3playergames.csv")
data_6player <- read_csv("ROS-Examples/Storable/data/6playergames.csv")
# combine them
data_all <-
  bind_rows(data_2player, data_3player, data_6player) %>% 
  # make an ordered factor version of the vote variable
  mutate(factor_vote = factor(vote, 
                              levels = 1:3, 
                              labels = c("1", "2", "3"), 
                              ordered = T))
# take a look
glimpse(data_all)
```



```{r}
data_all %>% 
  count(vote)
```

重复Figure 15.6，代码来源[ASKurz](https://github.com/ASKurz/Working-through-Regression-and-other-stories/blob/main/15.Rmd)

```{r, fig.width = 7, fig.height = 3.5}
# 6 participants
plotted <- c(101, 303, 409, 405, 504, 112)
# participant descriptors
story <- c("Perfectly monotonic", "One fuzzy and one sharp cutpoint", "Monotonic with one outlier", 
           "Only 1's and 3's", "Almost only 3's", "Erratic")
# wrangle
data_all %>% 
  filter(person %in% plotted) %>% 
  mutate(person = factor(person,
                         levels = plotted,
                         labels = story)) %>%
  # plot!
  ggplot(aes(x = value, y = vote)) +
  geom_point(alpha = 1/2) +
  scale_y_continuous(breaks = 1:3) +
  facet_wrap(~ person)
```
#### 15.5.3.2 Fitting the model in R

```{r, fig.width = 5, fig.height = 4}
data_all %>% 
  filter(person == 401) %>%
  ggplot(aes(x = value, y = vote)) +
  geom_point(alpha = 1/2) +
  scale_y_continuous(breaks = 1:3)
```

根据课本的公式 (15.10)
$$
\begin{align*}
\operatorname{Pr}(y_i = 1 | z_i) & = \operatorname{logit}^{-1}(c_1 - z_i) \\
\operatorname{Pr}(y_i = 2 | z_i) & = \operatorname{logit}^{-1}(c_2 - z_i) - \operatorname{logit}^{-1}(c_1 - z_i) \\
\operatorname{Pr}(y_i = 3 | z_i) & = 1 - \operatorname{logit}^{-1}(c_2 - z_i)
\end{align*}
$$

$$
\begin{align*}
\operatorname{Pr}(y_i = 1) & = \operatorname{logit}^{-1}(c_1 - X_i \beta) \\
\operatorname{Pr}(y_i = 2) & = \operatorname{logit}^{-1}(c_2 - X_i \beta) - \operatorname{logit}^{-1}(c_1 - X_i \beta) \\
\operatorname{Pr}(y_i = 3) & = 1 - \operatorname{logit}^{-1}(c_2 - X_i \beta)
\end{align*}
$$
Stan 代码
```{r, warning=FALSE, message=FALSE, results=FALSE}
stan_program <- "
data {
  int<lower=2> K;
  int<lower=0> N;
  int<lower=1> D;
  int<lower=1,upper=K> y[N];
  matrix[N, D] x;
}
parameters {
  vector[D] beta;
  ordered[K-1] cutpoint;
}
model {
  for (n in 1:N) {
    target += ordered_logistic_lpmf(y[n] | x[n] * beta, cutpoint);
  }
  //for (n in 1:N)
  //  y[n] ~ ordered_logistic(x[n] * beta, cutpoint);
  
  beta[1] ~ normal(0, 0.5);
}
"

stan_data <- data_all %>% 
  filter(person == 401) %>% 
  tidybayes::compose_data(
    N   = n,
    K   = 3,  
    D   = 1,
    y   = vote,  
    x   = model.matrix(~ 0 + value, .)
  )

m15.10 <- stan(model_code = stan_program, data = stan_data)
```


```{r}
m15.10
```

结果与课本不一样。我弱弱地观察到，课本貌似用的`data_all`，我们重点关注作者的分析过程。

```{r, eval=FALSE}
stan_data <- data_all %>% 
   tidybayes::compose_data(
    N   = n,
    K   = 3,  
    D   = 1,
    y   = vote,  
    x   = model.matrix(~ 0 + value, .)
  )

m15.10a <- stan(model_code = stan_program, data = stan_data)
```





```{r, fig.width = 7.5, fig.height = 2.25}
m15.10 %>% 
  tidybayes::spread_draws(beta[i], cutpoint[j]) %>% 
  ungroup() %>% 
  pivot_wider(
    values_from = cutpoint,
    names_from = j,
    names_glue = "cutpoint_{j}"
  ) %>% 
  transmute(`italic(c)[1.5]` = `cutpoint_1` / beta,
            `italic(c)[2.5]` = `cutpoint_2` / beta,
            sigma = 1 / beta) %>%
  pivot_longer(everything()) %>% 
  
  ggplot(aes(x = value, y = 0)) +
  stat_halfeye(.width = .95, normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("marginal posterior") +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```


#### 15.5.3.3 Displaying the fitted model.


```{r}
logit     <- qlogis
inv_logit <- plogis
```



根据如下公式，可计算 `y` 落入各种分类(1、2和3)的概率，这种概率是随着自变量的变化而变化的，
所以，ASKurz解释为概率轨迹(probability trajectories)


$$
\begin{align*}
\operatorname{Pr}(y_i = 1) & = \operatorname{logit}^{-1}(c_1 - X_i \beta) \\
\operatorname{Pr}(y_i = 2) & = \operatorname{logit}^{-1}(c_2 - X_i \beta) - \operatorname{logit}^{-1}(c_1 - X_i \beta) \\
\operatorname{Pr}(y_i = 3) & = 1 - \operatorname{logit}^{-1}(c_2 - X_i \beta),
\end{align*}
$$


图形如下:

```{r, warning = F, fig.width = 7, fig.height = 5}
m15.10 %>% 
  tidybayes::spread_draws(beta[i], cutpoint[j]) %>% 
  ungroup() %>% 
  pivot_wider(
    values_from = cutpoint,
    names_from = j,
    names_glue = "cutpoint_{j}"
  ) %>% 
  expand(
    nesting(.iteration, cutpoint_1, cutpoint_2, beta),
    value = 0:100
  ) %>% 
  mutate(mu = beta * value) %>%
  mutate(`1` = inv_logit(cutpoint_1 - mu),
         `2` = inv_logit(cutpoint_2 - mu) - inv_logit(cutpoint_1 - mu),
         `3` = 1 - inv_logit(cutpoint_2 - mu)
  ) %>% 
  pivot_longer(`1`:`3`, values_to = "prob", names_to = "vote") %>% 
  
  ggplot(aes(x = value, y = prob, color = vote, fill = vote)) +
  tidybayes::stat_lineribbon(.width = .95, size = 1/2, alpha = .4) +
  scale_fill_viridis_d(option = "A", end = .7) +
  scale_color_viridis_d(option = "A", end = .7) +
  scale_y_continuous(expression(italic(prob)), expand = c(0, 0), 
                     breaks = 0:5 / 5, limits = 0:1)
```


书中公式(15.11)

$$
\begin{align*}
\operatorname{E}(y | x) & = 1* \operatorname{Pr}(y = 1 |x) + 2* \operatorname{Pr}(y = 2 |x) + 3* \operatorname{Pr}(y = 3 |x) \\ 
& = 1 * ( 1 - \operatorname{logit}^{-1}(\frac{x - c_1}{\sigma})) \;+ \\
& \quad + 2 * ( \operatorname{logit}^{-1}(\frac{x - c_1}{\sigma})-  \operatorname{logit}^{-1}(\frac{x - c_2}{\sigma})) \\
& \quad + 3 *\operatorname{logit}^{-1}(\frac{x - c_2}{\sigma})
\end{align*}
$$

```{r}
f <- summary(m15.10)$summary[, 1]
f[2] / f[1]
f[3] / f[1]
```


```{r, fig.width = 5.5, fig.height = 5}
# for the cutpoints
lines <-
  tibble(value = rep(c(f[2] / f[1], f[3] / f[1]), each = 2),
         vote = c(1, 2, 2, 3))


m15.10 %>% 
  tidybayes::spread_draws(beta[i], cutpoint[j]) %>% 
  ungroup() %>% 
  pivot_wider(
    values_from = cutpoint,
    names_from = j,
    names_glue = "cutpoint_{j}"
  ) %>% 
  expand(
    nesting(.iteration, cutpoint_1, cutpoint_2, beta),
    value = 0:100
  ) %>% 
  mutate(mu = beta * value) %>%
  mutate(
    p1 = inv_logit(cutpoint_1 - mu),
    p2 = inv_logit(cutpoint_2 - mu) - inv_logit(cutpoint_1 - mu),
    p3 = 1 - inv_logit(cutpoint_2 - mu)
  ) %>% 
  mutate(vote = p1 * 1 + p2 * 2 + p3 * 3) %>% 
  
  
  # plot!
  ggplot(aes(x = value, y = vote)) +
  stat_lineribbon(.width = .95, size = 1/2, fill = "grey85") +
  geom_path(data = lines,
            aes(group = value),
            color = "red3") +
  geom_point(data = filter(data_all, person == 401),
             alpha = 1/2) +
  scale_y_continuous(breaks = 1:3) +
  ggtitle("Participant 401",
          subtitle = "One fuzzy and one sharp cutpoint") +
  theme(legend.position = c(.85, .2))
```



## 15.6 Robust regression using the $t$ model

```{r, warning = F, message = F}
congress <- read_csv("ROS-Examples/Congress/data/congress.csv")
head(congress)
```


## 15.8 Going beyond generalized linear models

```{r, warning = F, message = F}
library(tidyverse)
earnings <- read_csv("ROS-Examples/Earnings/data/earnings.csv")
glimpse(earnings)
```



$$
\begin{align*}
\log(\text{earn}^+) & \sim \operatorname{Normal}(\mu_i, \sigma_i) \\
\mu_i & = a + b_1 \text{height}_i + b_2 \text{male}_i \\
\log(\sigma_i) & = c + d_1 \text{male}_i,
\end{align*}
$$



```{r, warning=FALSE, message=FALSE}
stan_program <- "
data {
  int N;
  vector[N] y;
  vector[N] height;
  vector[N] male;
}
parameters {
  real a;
  real b1;
  real b2;
  real c;
  real d;
}
model {
  vector[N] mu;
  vector[N] sigma;
  
  for(i in 1:N) {
    mu[i] = a + b1 * height[i] + b2 * male[i];
    sigma[i] = exp(c + d * male[i]);
  }
  
  for(i in 1:N) {
    target += lognormal_lpdf(y[i] | mu[i], sigma[i]);
  }

}


"

d <- earnings %>% filter(earn > 0)

stan_data <- list(
   N      = nrow(d),
   y      = d$earn,
   height = d$height, 
   male   = d$male
  )

m15.19 <- stan(model_code = stan_program, data = stan_data)
```




```{r,  fig.width = 5, fig.height = 2.75}
m15.19 %>% 
  tidybayes::spread_draws(c, d) %>% 
  mutate(`sigma[male]`   = exp(c + d),
         `sigma[female]` = exp(c)) %>% 
  pivot_longer(`sigma[male]`:`sigma[female]`) %>% 
  
  ggplot(aes(x = value, y = name)) +
  stat_halfeye(.width = .95) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  coord_cartesian(ylim = c(1.5, 2.3))
```


### 15.8.2 Mixed discrete/continuous data.
在12章的时候，我们是讲其中`earning = 0`的去除后建立的模型。一般来说，我们可以将earning = 0的情形考虑进来。即，两个模型的混合

- 第一个 logistic regression 
$$
\begin{align*}
\text{Pr(earnings > 0)} &= \text{logit}^{-1}(\alpha + \beta\times x) \\
\end{align*}
$$

- 第二个 log normal regression

$$
\begin{align*}
\log(\text{earnings}) &= a + b x + \epsilon
\end{align*}
$$

$$
\begin{align}
y_n &\sim \operatorname{Lognormal}(\mu_n, \,\, \sigma)\\
\mu_n &= \alpha + \beta x_n 
\end{align}
$$
Stan代码分别如下
```{r, warning=FALSE, message=FALSE}
stan_program <- "
data {
  int<lower=0> N;
  int<lower=1> K; 
  matrix[N, K] X;
  int<lower=0,upper=1> y[N];
}
parameters {
  vector[K] beta; 
}
model {

  //for (n in 1:N){
  //  y[n] ~ bernoulli(inv_logit(X[n] * beta));
  //}
  
  // more efficient and arithmetically stable
  y ~ bernoulli_logit(X * beta);
}
"


stan_data <- earnings %>%
  tidybayes::compose_data(
    N      = n,
    K      = 3,
    y      = earn > 0,
    X      = model.matrix(~ 1 + height + male)
  )

m15.20 <- stan(model_code = stan_program, data = stan_data)
```

```{r}
m15.20
```


```{r, warning=FALSE, message=FALSE}
stan_program <- "
data {
  int<lower=0> N;
  int<lower=1> K; 
  matrix[N, K] X;
  vector[N] y;
}
parameters {
  vector[K] beta;  
  real<lower=0> sigma;
}
model {

   // y ~ lognormal(X * beta, sigma);
  
   for(i in 1:N) {
    target += lognormal_lpdf(y[i] | X[i] * beta, sigma);
   }
  
}
"


stan_data <- earnings %>%
  filter(earn > 0) %>% 
  tidybayes::compose_data(
    N      = n,
    K      = 3,
    y      = earn,
    X      = model.matrix(~ 1 + height + male)
  )

m15.21 <- stan(model_code = stan_program, data = stan_data)
```

```{r}
m15.21
```



### 15.8.4 Cockroaches and the zero-inflated negative binomial model.


$$
\begin{align*}
y_i & \sim \operatorname{negative\_binomial}(u_i \exp(X_i\beta), \phi) \\
& = \operatorname{negative\_binomial}(\exp(\log(u_i)X_i\beta), \;\phi) \\
& = \operatorname{negative\_binomial\_log}(\log(u_i)X_i\beta, \;\phi) \\
\log(u_i) & = \log(\text{exposure2}_i) \\
X_i\beta & = \beta_0 + \beta_1 \text{roach100}_i + \beta_2 \text{treatment}_i + \beta_3 \text{senior}_i 

\end{align*}
$$

零膨胀回归模型等价于混合模型

$$
\begin{align*}
y_i \begin{cases}
  =0,  & \text{ if } S_i = 0 \\
  \sim\operatorname{negative\_binomial}(u_i \exp(X_i\beta), \phi) & \text{ if } S_i = 1 \\
\end{cases}
\end{align*}
$$


这里的$S_i$ 代表房屋$i$是否有蟑螂，可以用logistic regression模拟
$$
\begin{align*}
\text{Pr}(S_i =1) = \text{logit}^{-1}(X_i \gamma),
\end{align*}
$$
但作者好像没有求$\gamma$，这里可以简化成
$$
\begin{align*}
\text{Pr}(S_i =1) = zi
\end{align*}
$$
即，$\zi$ 为零膨胀的概率。




```{r, warning = F}
roaches <- 
  read_csv("ROS-Examples/Roaches/data/roaches.csv", 
           col_types = cols(X1 = col_skip())) %>% 
  mutate(roach100 = roach1 / 100)

glimpse(roaches)
```


```{r}
stan_program <- "
functions {
  real zero_inflated_neg_binomial_log_lpmf(int y, real eta, real phi, real zi) { 
    if (y == 0) { 
      return log_sum_exp(bernoulli_lpmf(1 | zi), 
                         bernoulli_lpmf(0 | zi) + 
                         neg_binomial_2_log_lpmf(0 | eta, phi)); 
    } else { 
      return bernoulli_lpmf(0 | zi) + neg_binomial_2_log_lpmf(y | eta, phi); 
    } 
  } 
 
}
data {
  int<lower=1> N;  
  int Y[N];        
  int<lower=1> K;  
  matrix[N, K] X;  
  vector[N] offsets;

}

parameters {
  vector[K] b;              
  real<lower=0> shape;       // shape parameter
  real<lower=0,upper=1> zi;  // zero-inflation probability
}

model {

  vector[N] mu =  X * b + offsets;
  
  for (n in 1:N) {
     target += zero_inflated_neg_binomial_log_lpmf(Y[n] | mu[n], shape, zi);
  }

  target += gamma_lpdf(shape | 0.01, 0.01);
  target += beta_lpdf(zi | 1, 1);
  
}

"


stan_data <- roaches %>%
  tidybayes::compose_data(
    N      = n,
    K      = 4,
    Y      = y,
    X      = model.matrix(~ 1 + roach100 + treatment + senior),
    offsets = log(exposure2)
  )

m15.23a <- stan(model_code = stan_program, data = stan_data)
```

或者写在`model{}`里

```{r}
stan_program <- "
data {
  int<lower=1> N;  
  int y[N];        
  int<lower=1> K;  
  matrix[N, K] X;  
  vector[N] offsets;

}

parameters {
  vector[K] b;              
  real<lower=0> shape;       // shape parameter
  real<lower=0,upper=1> zi;  // zero-inflation probability
}

model {

  vector[N] mu = X * b + offsets;
  
  for (i in 1:N) {
   if (y[i] == 0) { 
      target += log_sum_exp(bernoulli_lpmf(1 | zi), 
                         bernoulli_lpmf(0 | zi) + 
                         neg_binomial_2_log_lpmf(0 | mu[i], shape)); 
    } else { 
      target += bernoulli_lpmf(0 | zi) + neg_binomial_2_log_lpmf(y[i] | mu[i], shape); 
    } 
  } 
 
 
  target += gamma_lpdf(shape | 0.01, 0.01);
  target += beta_lpdf(zi | 1, 1);
  
}


generated quantities {
  vector[N] y_predict; 

  for(n in 1:N) {
    real tmp = bernoulli_rng(zi);
    if (tmp == 1) {
      y_predict[n] = 0;
    } else {
      y_predict[n] = neg_binomial_2_log_rng(X[n] * b + offsets[n], shape);
    }
 }
   
}


"


stan_data <- roaches %>%
  tidybayes::compose_data(
    N      = n,
    K      = 4,
    y      = y,
    X      = model.matrix(~ 1 + roach100 + treatment + senior),
    offsets = log(exposure2)
  )

m15.23b <- stan(model_code = stan_program, data = stan_data)
```




```{r}
m15.23b
```



```{r, fig.width = 3, fig.height = 2.75, warning = F, message = F}
m15.23b %>% 
  tidybayes::spread_draws(y_predict[i]) %>% 
  mutate(y_rep_log = log10(y_predict + 1)) %>% 
  filter(.draw <= 200) %>% 
  ggplot(aes(x = y_rep_log)) +
  geom_density(aes(group = .draw), colour = alpha("gray", 0.3)) +
  geom_density(
    data = roaches, aes(x = log10(y + 1)), color = "red"
  ) +
  labs(subtitle = "zero-inflated negative binomial",
       x = "log10(y+1)") +
  coord_cartesian(xlim = c(NA, 3.25))
```


或者使用`bayesplot::ppc_dens_overlay()`

```{r, fig.width = 3, fig.height = 2.75, warning = F, message = F}
y_rep <- as.matrix(m15.23b, pars = "y_predict")  

bayesplot::ppc_dens_overlay(log10(stan_data$y + 1), log10(y_rep[1:200,] + 1) ) +

  labs(subtitle = "zero-inflated negative binomial",
       x = "log10(y+1)") +
  coord_cartesian(xlim = c(NA, 3.25))

```

### 15.8.5 Prediction and Bayesian inference.

15.8.2章节已经给出了两个模型`m15.20` 和`m15.21`的表达式

$$
\begin{align*}
y_i \begin{cases}
   \text{ Pr(earnings > 0) } &= \text{logit}^{-1}(-3.97 + 0.08*\text{height} + 1.70 * \text{male}) \\
   \text{If earnings > 0}, & \text{then earnings =} \exp(8.15 + 0.02* \text{height} + 0.43* \text{male} + \epsilon) \\
\end{cases}
\end{align*}
$$

这里的$\epsilon$ 服从`normal(0, 0.88)`，因此我们可以用`m15.20` 和`m15.21` 预测一位68inch的女性的收入情况。






